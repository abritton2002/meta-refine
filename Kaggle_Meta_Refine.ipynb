{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Meta-Refine on Kaggle\n",
    "\n",
    "## Running Meta Llama 3.1-8B for Claude Code-Level Analysis\n",
    "\n",
    "**Kaggle advantages:**\n",
    "- ‚úÖ 16GB RAM (vs 12GB Colab)\n",
    "- ‚úÖ 30 hours/week free GPU\n",
    "- ‚úÖ Better for ML workloads\n",
    "\n",
    "**Enable GPU:** Settings ‚Üí Accelerator ‚Üí GPU T4 x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check system resources\n",
    "!nvidia-smi\n",
    "!free -h\n",
    "print(\"GPU Memory and System RAM available for Meta Llama 8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo and install\n",
    "!git clone https://github.com/abritton2002/meta-refine.git\n",
    "import os\n",
    "os.chdir('/kaggle/working/meta-refine')\n",
    "\n",
    "!pip install -r requirements.txt\n",
    "!pip install accelerate bitsandbytes transformers torch --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Configure for Kaggle GPU environment\nimport os\n\n# IMPORTANT: Replace with your actual HF token\nHF_TOKEN = \"hf_YOUR_ACTUAL_TOKEN_HERE\"  # ‚ö†Ô∏è Replace this!\n\n# Kaggle-optimized settings\nos.environ['HF_TOKEN'] = HF_TOKEN\nos.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\n\nconfig = f\"\"\"\nHF_TOKEN={HF_TOKEN}\nMODEL_NAME=meta-llama/Llama-3.1-8B-Instruct\nMODEL_DEVICE=cuda\nMODEL_TEMPERATURE=0.3\nMODEL_LOAD_IN_4BIT=true\nTORCH_DTYPE=float16\nMAX_FILE_SIZE=1000000\nCHUNK_SIZE=2000\n\"\"\"\n\nwith open('.env', 'w') as f:\n    f.write(config)\n\nprint(\"‚úÖ Kaggle environment configured for Meta Llama 8B\")\nprint(\"üîß 4-bit quantization enabled for 16GB RAM\")\nprint(\"‚ö†Ô∏è  Remember to replace HF_TOKEN with your actual token!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Meta-Refine with Meta Llama\n",
    "print(\"üß† Loading Meta Llama 3.1-8B-Instruct...\")\n",
    "print(\"‚è≥ First run: Model download + load (~10-15 minutes)\")\n",
    "print(\"üíæ Using 4-bit quantization for Kaggle's 16GB RAM\")\n",
    "\n",
    "# Run analysis on test file\n",
    "!python -m meta_refine_pkg.cli analyze --file examples/test_code.py --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project-wide analysis\n",
    "print(\"üèóÔ∏è Analyzing entire Meta-Refine codebase...\")\n",
    "!python -m meta_refine_pkg.cli analyze --project meta_refine_pkg --format json --output kaggle_results.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View and download results\n",
    "import json\n",
    "import os\n",
    "\n",
    "if os.path.exists('kaggle_results.json'):\n",
    "    print(\"üìä Analysis Results Summary:\")\n",
    "    \n",
    "    with open('kaggle_results.json', 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    # Display summary\n",
    "    summary = results['meta_refine_analysis']['summary']\n",
    "    print(f\"Files analyzed: {summary['total_files']}\")\n",
    "    print(f\"Issues found: {summary['total_issues']}\")\n",
    "    print(f\"Analysis time: {summary['total_analysis_time']:.2f}s\")\n",
    "    \n",
    "    # Show severity breakdown\n",
    "    severity = summary.get('severity_breakdown', {})\n",
    "    print(f\"\\nüö® Issues by severity:\")\n",
    "    for sev, count in severity.items():\n",
    "        print(f\"  {sev}: {count}\")\n",
    "    \n",
    "    # Show sample issues\n",
    "    print(f\"\\nüìã Sample Issues Found:\")\n",
    "    for file_result in results['meta_refine_analysis']['files'][:3]:\n",
    "        if file_result['issues']:\n",
    "            print(f\"\\nüìÅ {file_result['file_path']}:\")\n",
    "            for issue in file_result['issues'][:2]:\n",
    "                print(f\"  ‚Ä¢ {issue['severity']}: {issue['description'][:80]}...\")\n",
    "                \n",
    "else:\n",
    "    print(\"‚ùå No results file found - check if analysis completed successfully\")\n",
    "    print(\"üìã Available files:\")\n",
    "    !ls -la *.json 2>/dev/null || echo \"No JSON files found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test custom code analysis\n",
    "print(\"üîß Test with your own code:\")\n",
    "print(\"Copy-paste your code below and run analysis\")\n",
    "\n",
    "# Example: Analyze custom code\n",
    "custom_code = \"\"\"\n",
    "def risky_function():\n",
    "    password = \"secret123\"  # Hardcoded secret\n",
    "    user_input = input(\"Enter SQL: \")\n",
    "    query = f\"SELECT * FROM users WHERE id = {user_input}\"  # SQL injection\n",
    "    return eval(user_input)  # Code injection\n",
    "\"\"\"\n",
    "\n",
    "# Save and analyze\n",
    "with open('custom_test.py', 'w') as f:\n",
    "    f.write(custom_code)\n",
    "\n",
    "print(\"\\nüîç Analyzing custom code with Meta Llama...\")\n",
    "!python -m meta_refine_pkg.cli analyze --file custom_test.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}