{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧪 Meta-Refine Simple Test\n",
    "\n",
    "Since ngrok requires authentication, let's test Meta-Refine directly in Colab.\n",
    "This will prove the system works with Meta Llama before setting up networking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi\n",
    "import torch\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Clone repo and install\n!git clone https://github.com/abritton2002/meta-refine.git\n%cd meta-refine\n!pip install -r requirements.txt\n!pip install accelerate bitsandbytes"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Configure for GPU + quantization\nimport os\n\n# IMPORTANT: Replace with your actual HF token\nHF_TOKEN = \"hf_YOUR_TOKEN_HERE\"  # Replace with your actual token\n\nos.environ['HF_TOKEN'] = HF_TOKEN\nos.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'\n\nconfig = f\"\"\"\nHF_TOKEN={HF_TOKEN}\nMODEL_NAME=meta-llama/Llama-3.1-8B-Instruct\nMODEL_DEVICE=auto\nMODEL_TEMPERATURE=0.3\nMODEL_LOAD_IN_4BIT=true\n\"\"\"\n\nwith open('.env', 'w') as f:\n    f.write(config)\n\nprint(\"✅ Configuration ready for GPU + 4-bit quantization\")\nprint(\"⚠️  Remember to replace HF_TOKEN with your actual token!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Meta-Refine CLI directly in Colab\n",
    "print(\"🧪 Testing Meta-Refine with Meta Llama...\")\n",
    "print(\"⏳ First run will download model (~5-10 minutes)\")\n",
    "\n",
    "# Run the CLI analysis\n",
    "!python -m meta_refine_pkg.cli analyze --file examples/test_code.py --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test project analysis\n",
    "print(\"🏗️ Testing project-wide analysis...\")\n",
    "!python -m meta_refine_pkg.cli analyze --project meta_refine_pkg --format json --output colab_results.json"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Download results\nfrom google.colab import files\nimport os\n\nprint(\"📥 Download your analysis results:\")\n\n# Check if file exists first\nif os.path.exists('colab_results.json'):\n    try:\n        files.download('colab_results.json')\n        print(\"✅ Results downloaded\")\n    except:\n        print(\"📄 Showing results inline:\")\n        with open('colab_results.json', 'r') as f:\n            content = f.read()\n            print(content[:2000])\n            if len(content) > 2000:\n                print(\"... (truncated)\")\nelse:\n    print(\"❌ Results file not found\")\n    print(\"📋 Checking what files were created:\")\n    !ls -la\n    \n    print(\"\\n🔍 Checking if analysis ran successfully:\")\n    print(\"Try running the analysis command again manually if needed\")"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}